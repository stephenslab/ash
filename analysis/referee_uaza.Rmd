---
title: "Assessment of referee comment regarding q value and zero assumption"
author: "Matthew Stephens"
date: 2016-03-21
---

**Last updated:** `r Sys.Date()`

**Code version:** `r system("git log -1 --format='%H'", intern = TRUE)`

```{r chunk-options, include=FALSE}
source("chunk-options.R")
```

## Simulation

A referee reported a simulation of 10,000 hypotheses, a fraction 0.8 are null ($\mu_i=0$), and a fraction 0.2 are alternative with effect $\mu_i \sim N(0,1)$. For each effect $i$ there
are n=30 observations from $x_{i,j} \sim N(\mu_i,1)$ and a one-sample t test is used to compute a $p$ value. The referee correctly notes that these simulations i) obey the
unimodal assumption (UA), and ii) have the property that "most" $z$ scores near 0 are null (Efron's "Zero Assumption"; ZA).
The motivation is to demonstrate that the UA and the ZA are not contradictory.

I agree with this, if we define the ZA as "most" $z$ scores near 0 are null.
However, in practice the existing methods effectively make a stronger assumption
(or at least, behave as if they do): that *all* $z$ scores near 0 are null.
Consequently they end up creating a hole in the distribution of alternative $z$ scores at 0. Here we illustrate this by applying qvalue to this scenario.

```{r}
set.seed(100)
mu = c(rep(0,8000),rnorm(2000))
x = matrix(rnorm(30*10000),nrow=10000,ncol=30)+mu
ttest.pval = function(x){return(t.test(x)$p.value)}
x.p = apply(x,1,ttest.pval)
```



Here is how qvalue (arguably, implicitly) decomposes the distribution of p values into null and alternative
```{r}
source("../R/nullalthist.R")
lfdr.qv = qvalue::qvalue(x.p)$lfdr
altnullhist(x.p,lfdr.qv,main="p values: qvalue",ncz=40,xlab="p value",cex.axis=0.8)
```


And here is the corresponding plot in z score space. Note the "hole" in the distribution of $z$ scores at 0 under the alternative.
```{r}
ttest.est = function(x){return(t.test(x)$estimate)}
x.est = apply(x,1,ttest.est)
zscore = x.est/(1/sqrt(30))
nullalthist(zscore,lfdr.qv,main="qvalue's implicit partition of z \n into null and alternative",ncz=60,cex.axis=0.8,ylim=c(0,0.3),xlim=c(-6,6),xlab="z score")
```

For comparison here is the decomposition, using ash, of $z$ scores into null and alternative components.
```{r}
library(ashr)
lfdr.ash=get_lfdr(ash(x.est,1/sqrt(30)))
nullalthist(zscore,lfdr.ash,main="ash partition of z \n into null and alternative",ncz=60,cex.axis=0.8,ylim=c(0,0.3),xlim=c(-6,6),xlab="z score")
```

For completeness here is (roughly) the ``true" decomposition of $z$ scores into null and alternative components.
```{r}
trueg = ashr::normalmix(c(0.8,0.2),c(0,0),c(0,1))
lfdr.true=get_lfdr(ash(x.est,1/sqrt(30),g=trueg,fixg=TRUE))
nullalthist(zscore,lfdr.true,main="true partition of z \n into null and alternative",ncz=60,cex.axis=0.8,ylim=c(0,0.3),xlim=c(-6,6),xlab="z score")
```

## Explanation

In estimating $\pi_0$ qvalue assumes that *all* p values near 1 are null.
This is equivalent to assuming that *all* z scores near 0 are null. Thus,
despite the fact that qvalue does not explicitly model the distribution of
$p$ values or $z$ scores under the alternative, its way of estimating $\pi_0$
necessarily creates a hole in the $z$ score distribution at 0 under the alternative.


## Session information

```{r info}
sessionInfo()
```
